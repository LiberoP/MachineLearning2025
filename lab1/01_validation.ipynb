{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine Learning LAB 1: MODEL SELECTION\n",
    "\n",
    "Course 2025/26: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement **MODEL SELECTION AND VALIDATION**.\n",
    "\n",
    "Complete all the **required code sections** and **answer all the questions**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Classification on Signal to Noise Ratios\n",
    "\n",
    "In this notebook we are going to explore the use of polynomial classification with polynomial regression. We are going to use the Numpy **polyfit** function, which performs polynomial regression.\n",
    "\n",
    "Our use case is a communication problem: we have a set of measurements of the Signal to Noise Ratio (SNR), i.e., the quality of the communication link, in various positions. The SNR depends on two components: firstly, the noise level (which is a random variable that does not depend on position) and the signal attenuation (usually modeled as a polynomial function of the distance).\n",
    "\n",
    "Our transmitter is in (0,0), and coordinates are in meters. In urban scenarios, the attenuation usually follows a third-degree polynomial, but it might be a fourth- or fifth-degree polynomial in more complex cases. How do we choose between different degrees? We will try with a maximum of **6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> warning: in this notebook, numpy is used as little as possible.\n",
    "# As an exercise one could try to implement everything without Numpy, but this would mean having to change list handling inside function definitions.\n",
    "# It would instead be more interesting to implement np.polyfit by hand, ie with linear algebra.\n",
    "# Minor cleanups also needed here.\n",
    "import numpy as np\n",
    "#import scipy as sp\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "In this case, x and y are the two coordinates, and the SNR is the thing we are trying to predict\n",
    "\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/snr_measurements.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/snr_measurements.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/snr_measurements.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/snr_measurements.csv',sep=';')\n",
    "x = df['x'].to_numpy()\n",
    "y = df['y'].to_numpy()\n",
    "#print(type(x))\n",
    "SNR = df['SNR'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(x,y)\n",
    "d_data=np.sqrt(x**2 + y**2) # ---> distance data\n",
    "plt.scatter(x=d_data, y=SNR, label='Data')\n",
    "plt.xlabel('distance from origin')\n",
    "plt.ylabel('SNR measurement')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These functions will help us evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = [1,2,3,7,8,9,10]\n",
    "#b= [111,112,113,117,118,119,1110]\n",
    "#for i in range(len(a)):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(x1, x2):\n",
    "    # Euclidean distance from the origin of a vector of components x1, x2\n",
    "    # check that floats?\n",
    "    distance = np.sqrt(x1**2 + x2**2)\n",
    "    return distance\n",
    "   \n",
    "#print(compute_distance(0,-0.01))\n",
    "\n",
    "def fit(distance, SNR, degree):\n",
    "    return np.polyfit(x=distance, y=SNR, deg=degree) # least squares polynomial fit, SNR as funct of distance from origin of (x,y) \n",
    "    # returns coefficients highest to lowest in a np array\n",
    "\n",
    "def predict(distance, poly_coeffs):\n",
    "    # Predict the SNR from a given model \n",
    "    # (of single distance-point for given coeffs)\n",
    "    acc=0\n",
    "    size = np.size(poly_coeffs)\n",
    "    degree = size -1\n",
    "    for i in range(size):\n",
    "        acc += poly_coeffs[degree-i] * distance**i\n",
    "    SNR = acc\n",
    "    return SNR\n",
    "    \n",
    "#a=np.array([1,1,4]) # deg 2, size 3 -> i = 0,1,2\n",
    "#print(a[1])\n",
    "#print(predict(1,a))\n",
    "\n",
    "def evaluate(distance, SNR, poly_coeffs): # ---> NB distance and SNR here are arrays of the same lenght\n",
    "    # Compute the error of the polynomial fit on the chosen data\n",
    "    mse = 0\n",
    "    for i in range(len(distance)):\n",
    "        predicted_SNR=predict(distance[i], poly_coeffs)\n",
    "        mse += (SNR[i]-predicted_SNR)**2\n",
    "    return mse/len(distance)\n",
    "    \n",
    "#ev = evaluate(\n",
    "#print(ev)\n",
    "\n",
    "def separate_test(distance, SNR, test_points):\n",
    "    # Return a training set and a test set (the test_points parameter controls the number of test points).\n",
    "    # The points should be selected randomly \n",
    "    indexes = list(range(len(distance))) # ---> NB range return type is different from list\n",
    "    test_index = random.sample(population = indexes, k = test_points) # ---> returns list\n",
    "    train_index = [i for i in indexes if i not in test_index]\n",
    "    x_test = distance[test_index]\n",
    "    # ---> NB works fine even tho they're both lists, no need for \"for i in indexes: x_test.append(distance[i])\"\n",
    "    # NB bc must select the CORRESPONDING x and y 's\n",
    "    y_test = SNR[test_index] \n",
    "    x_train = distance[train_index]\n",
    "    y_train = SNR[train_index]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test # ---> returns tuple\n",
    "\n",
    "#separate_test(a, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the SNR as a function of the distance\n",
    "#plt.scatter(x,y)\n",
    "\n",
    "#N = int(0.2 * len(d)) # ---> returns integer part of number (ie floor)\n",
    "d= compute_distance(x,y) \n",
    "N_test_points=40\n",
    "distance_separated = separate_test(distance = d, SNR = SNR, test_points = N_test_points)\n",
    "\n",
    "plt.scatter(x=distance_separated[0], y=distance_separated[1], label='Training data', color = 'b')\n",
    "plt.scatter(x=distance_separated[2], y=distance_separated[3], label='Test data', color= 'r')\n",
    "plt.xlabel('distance from origin')\n",
    "plt.ylabel('SNR measurement')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. K-fold cross-validation\n",
    "\n",
    "In this case, x and y are the two coordinates, and the SNR is the thing we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(range(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the K-fold cross validation\n",
    "def k_fold_cross_validation(x_train: np.ndarray, y_train: np.ndarray, max_degree: int, k: int) -> tuple[tuple, tuple]:\n",
    "    # TODO: Perform K-fold cross-validation on the training set.\n",
    "    # The two returned values are the best model and the list of results for all degrees up to max_degree.\n",
    "    # The points should be selected randomly.\n",
    "    # The inputs and labels are already in terms of distance and SNR\n",
    "\n",
    "    results = [[],[]]\n",
    "    mse_perf= 1e9\n",
    "    # split in max_degree+1 folds:\n",
    "   \n",
    "    for n in range(max_degree+1):\n",
    "        original_indexes = list(range(len(x_train))) \n",
    "        new_indexes = original_indexes\n",
    "        N_in_fold = len(original_indexes) // k\n",
    "        indexes = random.sample(population = new_indexes, k=N_in_fold) \n",
    "            indexes_list.append(indexes)\n",
    "            new_indexes = [x for x in original_indexes if x not in indexes]\n",
    "    \n",
    "        # create a list of max_degree+1 lists of indexes, ie split the original list of indexes randomly\n",
    "        indexes_list = []\n",
    "        #print(indexes_list)\n",
    "        mse_list=[]\n",
    "        # create model for each degree:\n",
    "        for i in range(k):\n",
    "            validation_indexes = indexes_list[i]\n",
    "            #print(validation_indexes)\n",
    "            training_indexes = []\n",
    "            for fold in indexes_list: # eg 1st,2nd,3rd,4th lists\n",
    "                    if fold != validation_indexes:\n",
    "                        training_indexes.extend(fold)\n",
    "            #print(training_indexes)\n",
    "            validation_set_x = x_train[validation_indexes]\n",
    "            #print(validation_set_x)\n",
    "            validation_set_y = y_train[validation_indexes]\n",
    "            training_set_x = x_train[training_indexes]\n",
    "            training_set_y = y_train[training_indexes]\n",
    "            #print(validation_set_y)\n",
    "            coefficients = fit(training_set_x, training_set_y, n)\n",
    "            #print(coefficients)\n",
    "            #predicted_y = predict(validation_set_x, coefficients) # ---> predicted y on validation set\n",
    "            #print(predicted_y)\n",
    "            mse = evaluate(validation_set_x, validation_set_y, coefficients)\n",
    "            mse_list.append(mse)\n",
    "            #print(mse)\n",
    "        # averaged_mse = np.average(mse_list) \n",
    "        averaged_mse = 0\n",
    "        for i in mse_list:\n",
    "            averaged_mse += i\n",
    "        averaged_mse /= k\n",
    "        results[0].append(coefficients)\n",
    "        results[1].append(averaged_mse)\n",
    "        if averaged_mse < mse_perf:    \n",
    "            best = fit(x_train, y_train, max_degree) # ---> NB retrain on the whole set \n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training with K-fold cross-validation with 40 test points and 4 folds Plot the validation score as a function of the degree\n",
    "md = 5\n",
    "n_folds = 4\n",
    "kfold_res = k_fold_cross_validation(x_train = d, y_train = SNR, max_degree= md, k = n_folds)\n",
    "\n",
    "deglist= list(range(md+1))\n",
    "#print(deglist)\n",
    "#print(res[0], '\\n', res[1])\n",
    "plt.plot(deglist, res[1][1])\n",
    "plt.xlabel('degree of polynomial')\n",
    "plt.ylabel('MSE on validation set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test performance of the best model and plot the model output and test points. \n",
    "# Try running the program multiple times, changing the values of K and the number of test points: is the output always the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(0,150,num=1000)\n",
    "predicted_kfold_best = predict(x_values, kfold_res[0])\n",
    "#evaluated_best = evaluate(distance_separated[2], distance_separated[3], res[0])\n",
    "#print(evaluated_best)\n",
    "#print(distance_separated[2], '\\n', predicted_best)\n",
    "plt.scatter(x=distance_separated[0], y=distance_separated[1], label='Training data', color = 'b')\n",
    "plt.scatter(x=distance_separated[2], y=distance_separated[3], label='Test data', color= 'r')\n",
    "plt.plot(x_values, predicted_kfold_best, label='Model')\n",
    "plt.xlabel('distance from origin')\n",
    "plt.ylabel('SNR measurement')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Tikhonov regularization\n",
    "\n",
    "Change the loss function to include a Tikhonov regularization term, as an alternative to cross-validation (try $\\lambda=0.01$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_norm(array): # python or np 1d array, varying return type\n",
    "    acc=0\n",
    "    for x in array: acc+=x*x\n",
    "    return acc\n",
    "\n",
    "def average_of_components(array):\n",
    "    acc = 0\n",
    "    for i in range(len(array)):\n",
    "        acc += array[i]\n",
    "    acc /= len(array)\n",
    "    return acc\n",
    "\n",
    "#average_of_components(np.array([4,2,1]))\n",
    "#squared_norm(np.array([1,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tikhonov(x_train: np.ndarray, y_train: np.ndarray, lambda_par: float, max_degree: int) -> tuple[tuple, tuple]:\n",
    "    # apply Tikhonov regularization AFTER the fitting process\n",
    "    # ---> NB no averages needed!\n",
    "    results = [[],[]]\n",
    "    loss_perf= 1e9\n",
    "    # create model for each degree:\n",
    "    for n in range(max_degree+1):\n",
    "        coefficients = fit(x_train, y_train, n)\n",
    "        mse = evaluate(x_train, y_train, coefficients) \n",
    "        loss = mse + lambda_par * squared_norm(coefficients)\n",
    "        results[0].append(coefficients)\n",
    "        results[1].append(loss)\n",
    "    if loss < loss_perf:\n",
    "        best = coefficients   \n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training with Tikhonov regularization and plot the loss as a function of the degree\n",
    "#md = 5\n",
    "#n_test_points = 40\n",
    "lbd_tik = 0.01\n",
    "#distance = compute_distance(x, y)\n",
    "\n",
    "#x_train, y_train, x_test, y_test = separate_test(distance, SNR, n_test_points)\n",
    "tik_res = evaluate_tikhonov(x_train = x_train, y_train = y_train, max_degree= md, lambda_par = lbd_tik)\n",
    "#print(res[1][1])\n",
    "\n",
    "#deglist= list(range(md+1))\n",
    "plt.plot(deglist, tik_res[1][1])\n",
    "plt.xlabel('degree of polynomial')\n",
    "plt.ylabel('regularized MSE (Tikhonov) on training set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Minimum description length regularization\n",
    "\n",
    "Change the loss function to include a representation length regularization term, as an alternative to cross-validation. The minimum description length of a polynomial of degree N is O(2^N) - try $\\lambda=0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_representation(x_train: np.ndarray, y_train: np.ndarray, lambda_par: float, max_degree: int) -> tuple[tuple, tuple]:\n",
    "    # apply MDL regularization AFTER the fitting process\n",
    "    results = [[],[]]\n",
    "    loss_perf= 1e9\n",
    "    # create model for each degree:\n",
    "    for n in range(max_degree+1):\n",
    "        coefficients = fit(x_train, y_train, n)\n",
    "        mse = evaluate(x_train, y_train, coefficients) \n",
    "        loss = mse + lambda_par * 2**max_degree\n",
    "        results[0].append(coefficients)\n",
    "        results[1].append(loss)\n",
    "    if loss < loss_perf:\n",
    "        best = coefficients   \n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training with MDL regularization and plot the loss as a function of the degree\n",
    "#md = 5\n",
    "#n_test_points = 40\n",
    "lbd_mdl = 0.02\n",
    "#distance = compute_distance(x, y)\n",
    "\n",
    "#x_train, y_train, x_test, y_test = separate_test(distance, SNR, n_test_points)\n",
    "mdl_res = evaluate_representation(x_train = x_train, y_train = y_train, max_degree= md, lambda_par = lbd_mdl)\n",
    "\n",
    "#deglist= list(range(md+1))\n",
    "plt.plot(deglist, mdl_res[1][1])\n",
    "plt.xlabel('degree of polynomial')\n",
    "plt.ylabel('regularized MSE (min. descr. length on training set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST\n",
    "\n",
    "Check the performance of the three solutions on the test set: which one does best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_mse = evaluate(x_test, y_test, kfold_res[0])\n",
    "tik_mse = evaluate(x_test, y_test, tik_res[0])\n",
    "mdl_mse = evaluate(x_test, y_test, mdl_res[0])\n",
    "print(f\"Each MSE is: {kfold_mse} for k-fold cross validation, {tik_mse} for Tikhonov regularisation, and {mdl_mse} for Minimum description length regularisation\")\n",
    "\n",
    "predicted_kfold_best = predict(x_values, kfold_res[0])\n",
    "predicted_tik_best = predict(x_values, tik_res[0])\n",
    "predicted_mdl_best = predict(x_values, mdl_res[0])\n",
    "plt.scatter(x=distance_separated[2], y=distance_separated[3], label='Test data', color= 'r')\n",
    "predicted_tik_best = predict(x_values, tik_res[0])\n",
    "predicted_mdl_best = predict(x_values, mdl_res[0])\n",
    "plt.plot(x_values, predicted_kfold_best, label='Best k-fold model', color = 'y')\n",
    "plt.plot(x_values, predicted_tik_best, label='Best Tikhonov model', color = 'b')\n",
    "plt.plot(x_values, predicted_mdl_best, label='Best MDL model', color = 'g')\n",
    "plt.xlabel('distance from origin')\n",
    "plt.ylabel('SNR measurement')\n",
    "plt.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
